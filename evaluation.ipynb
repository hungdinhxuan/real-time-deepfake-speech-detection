{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# LA19"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>LA_E_ID</th>\n",
                            "      <th>Index</th>\n",
                            "      <th>Key</th>\n",
                            "      <th>Code</th>\n",
                            "      <th>Label</th>\n",
                            "      <th>Score</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>LA_T_1000137_0</td>\n",
                            "      <td>LA_0094</td>\n",
                            "      <td>-</td>\n",
                            "      <td>A04</td>\n",
                            "      <td>spoof</td>\n",
                            "      <td>-3.301523</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>LA_T_1000137_1</td>\n",
                            "      <td>LA_0094</td>\n",
                            "      <td>-</td>\n",
                            "      <td>A04</td>\n",
                            "      <td>spoof</td>\n",
                            "      <td>-3.309291</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>LA_T_1000137_2_no_speech_residual</td>\n",
                            "      <td>LA_0094</td>\n",
                            "      <td>-</td>\n",
                            "      <td>A04</td>\n",
                            "      <td>spoof</td>\n",
                            "      <td>-3.214029</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>LA_T_1000406_0_no_speech</td>\n",
                            "      <td>LA_0089</td>\n",
                            "      <td>-</td>\n",
                            "      <td>-</td>\n",
                            "      <td>bonafide</td>\n",
                            "      <td>3.281868</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>LA_T_1000406_1</td>\n",
                            "      <td>LA_0089</td>\n",
                            "      <td>-</td>\n",
                            "      <td>-</td>\n",
                            "      <td>bonafide</td>\n",
                            "      <td>3.135463</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "                             LA_E_ID    Index Key Code     Label     Score\n",
                            "0                     LA_T_1000137_0  LA_0094   -  A04     spoof -3.301523\n",
                            "1                     LA_T_1000137_1  LA_0094   -  A04     spoof -3.309291\n",
                            "2  LA_T_1000137_2_no_speech_residual  LA_0094   -  A04     spoof -3.214029\n",
                            "3           LA_T_1000406_0_no_speech  LA_0089   -    -  bonafide  3.281868\n",
                            "4                     LA_T_1000406_1  LA_0089   -    -  bonafide  3.135463"
                        ]
                    },
                    "execution_count": 31,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import pandas as pd\n",
                "\n",
                "# Assuming these are the file paths where your data is stored\n",
                "eval_score_path = '/datad/hungdx/Rawformer-implementation-anti-spoofing/aasist_runs/XLSR_AASIST_RawBoost3/scores/XLSR_AASIST_DA3_EMPHASIS_LA19train_score_1s_baseline_newdataset_librosa_bestDF.txt'\n",
                "asv_score_file = '/datad/hungdx/Rawformer-implementation-anti-spoofing/datasets/protocols/LA19.cm.train.trn_1s.txt'\n",
                "\n",
                "# Load the data\n",
                "cm_data = pd.read_csv(asv_score_file, sep=' ', header=None)\n",
                "submission_scores = pd.read_csv(\n",
                "    eval_score_path, sep=' ', header=None, index_col=0)\n",
                "\n",
                "# Assuming that the second column in cm_data DataFrame contains the IDs that should match\n",
                "# with the index in submission_scores DataFrame, we set this column as the index.\n",
                "cm_data.set_index(1, inplace=True)\n",
                "\n",
                "# Now, we join the DataFrames on the index (which is the common 'LA_E' ID)\n",
                "# We also sort the index to ensure the order is correct after join\n",
                "merged_df = cm_data.join(submission_scores, how='inner').sort_index()\n",
                "\n",
                "# Reset index to get the 'LA_E' ID as a column again if needed\n",
                "merged_df.reset_index(inplace=True)\n",
                "\n",
                "# Rename the columns to have meaningful names\n",
                "merged_df.columns = ['LA_E_ID', 'Index', 'Key', 'Code', 'Label', 'Score']\n",
                "\n",
                "# Let's see the first few rows of the merged dataframe\n",
                "merged_df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "EER: 22.24% at threshold 1.38\n"
                    ]
                }
            ],
            "source": [
                "from evaluation import compute_eer\n",
                "bona_cm = merged_df[merged_df[\"Label\"] == 'bonafide']['Score'].values\n",
                "spoof_cm = merged_df[merged_df[\"Label\"] == 'spoof']['Score'].values\n",
                "\n",
                "eer_cm, th = compute_eer(bona_cm, spoof_cm)\n",
                "\n",
                "print(f'EER: {eer_cm:.2%} at threshold {th:.2f}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## no silence subset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "EER: 4.80% at threshold -3.13\n"
                    ]
                }
            ],
            "source": [
                "from evaluation import compute_eer\n",
                "bona_cm = merged_df[(merged_df[\"Label\"] == 'bonafide') &\n",
                "                    (~merged_df[\"LA_E_ID\"].str.contains('no_speech'))]['Score'].values\n",
                "spoof_cm = merged_df[(merged_df[\"Label\"] == 'spoof') &\n",
                "                     (~merged_df[\"LA_E_ID\"].str.contains('no_speech'))]['Score'].values\n",
                "\n",
                "eer_cm, th = compute_eer(bona_cm, spoof_cm)\n",
                "\n",
                "print(f'EER: {eer_cm:.2%} at threshold {th:.2f}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## no residual subset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "EER: 16.61% at threshold 1.77\n"
                    ]
                }
            ],
            "source": [
                "from evaluation import compute_eer\n",
                "bona_cm = merged_df[(merged_df[\"Label\"] == 'bonafide') &\n",
                "                    (~merged_df[\"LA_E_ID\"].str.contains('residual'))]['Score'].values\n",
                "spoof_cm = merged_df[(merged_df[\"Label\"] == 'spoof') &\n",
                "                     (~merged_df[\"LA_E_ID\"].str.contains('residual'))]['Score'].values\n",
                "\n",
                "eer_cm, th = compute_eer(bona_cm, spoof_cm)\n",
                "\n",
                "print(f'EER: {eer_cm:.2%} at threshold {th:.2f}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## no residual subset and no silence subset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "EER: 1.49% at threshold -2.60\n"
                    ]
                }
            ],
            "source": [
                "from evaluation import compute_eer\n",
                "bona_cm = merged_df[(merged_df[\"Label\"] == 'bonafide') &\n",
                "                    (~merged_df[\"LA_E_ID\"].str.contains('residual')) &\n",
                "                    (~merged_df[\"LA_E_ID\"].str.contains('no_speech'))\n",
                "                    ]['Score'].values\n",
                "spoof_cm = merged_df[(merged_df[\"Label\"] == 'spoof') &\n",
                "                     (~merged_df[\"LA_E_ID\"].str.contains('residual')) &\n",
                "                     (~merged_df[\"LA_E_ID\"].str.contains('no_speech'))\n",
                "                     ]['Score'].values\n",
                "\n",
                "eer_cm, th = compute_eer(bona_cm, spoof_cm)\n",
                "\n",
                "print(f'EER: {eer_cm:.2%} at threshold {th:.2f}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## no silence residual subset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "EER: 15.93% at threshold 0.44\n"
                    ]
                }
            ],
            "source": [
                "from evaluation import compute_eer\n",
                "bona_cm = merged_df[(merged_df[\"Label\"] == 'bonafide') &\n",
                "                    (~merged_df[\"LA_E_ID\"].str.contains('no_speech_residual'))\n",
                "                    ]['Score'].values\n",
                "spoof_cm = merged_df[(merged_df[\"Label\"] == 'spoof') &\n",
                "                     (~merged_df[\"LA_E_ID\"].str.contains('no_speech_residual'))]['Score'].values\n",
                "\n",
                "eer_cm, th = compute_eer(bona_cm, spoof_cm)\n",
                "\n",
                "print(f'EER: {eer_cm:.2%} at threshold {th:.2f}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# LA19 eval"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>LA_E_ID</th>\n",
                            "      <th>Index</th>\n",
                            "      <th>Key</th>\n",
                            "      <th>Code</th>\n",
                            "      <th>Label</th>\n",
                            "      <th>Score</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>LA_E_1000147</td>\n",
                            "      <td>LA_0044</td>\n",
                            "      <td>-</td>\n",
                            "      <td>A10</td>\n",
                            "      <td>spoof</td>\n",
                            "      <td>0.681554</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>LA_E_1000273</td>\n",
                            "      <td>LA_0001</td>\n",
                            "      <td>-</td>\n",
                            "      <td>A15</td>\n",
                            "      <td>spoof</td>\n",
                            "      <td>-4.100327</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>LA_E_1000791</td>\n",
                            "      <td>LA_0023</td>\n",
                            "      <td>-</td>\n",
                            "      <td>A11</td>\n",
                            "      <td>spoof</td>\n",
                            "      <td>0.419275</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>LA_E_1000841</td>\n",
                            "      <td>LA_0043</td>\n",
                            "      <td>-</td>\n",
                            "      <td>A09</td>\n",
                            "      <td>spoof</td>\n",
                            "      <td>-5.771040</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>LA_E_1000989</td>\n",
                            "      <td>LA_0014</td>\n",
                            "      <td>-</td>\n",
                            "      <td>A14</td>\n",
                            "      <td>spoof</td>\n",
                            "      <td>-5.543950</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "        LA_E_ID    Index Key Code  Label     Score\n",
                            "0  LA_E_1000147  LA_0044   -  A10  spoof  0.681554\n",
                            "1  LA_E_1000273  LA_0001   -  A15  spoof -4.100327\n",
                            "2  LA_E_1000791  LA_0023   -  A11  spoof  0.419275\n",
                            "3  LA_E_1000841  LA_0043   -  A09  spoof -5.771040\n",
                            "4  LA_E_1000989  LA_0014   -  A14  spoof -5.543950"
                        ]
                    },
                    "execution_count": 1,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import pandas as pd\n",
                "\n",
                "# Assuming these are the file paths where your data is stored\n",
                "eval_score_path = '/home/hungdx/code/Towards-Real-Time-Deepfake-Speech-Detection-in-Resource-Limited-Scenarios/runs/aasist_runs/XLSR_AASIST_RawBoost4/scores/random/la19/XLSR_AASIST_DA3_EMPHASIS_LA19_score_epoch32_0.004826_99.6229.pt.txt'\n",
                "asv_score_file = '/datad/hungdx/KDW2V-AASISTL/protocols/ASVspoof_LA_cm_protocols/ASVspoof2019.LA.cm.eval.trl.txt'\n",
                "\n",
                "# Load the data\n",
                "cm_data = pd.read_csv(asv_score_file, sep=' ', header=None)\n",
                "submission_scores = pd.read_csv(\n",
                "    eval_score_path, sep=' ', header=None, index_col=0)\n",
                "\n",
                "# Assuming that the second column in cm_data DataFrame contains the IDs that should match\n",
                "# with the index in submission_scores DataFrame, we set this column as the index.\n",
                "cm_data.set_index(1, inplace=True)\n",
                "\n",
                "# Now, we join the DataFrames on the index (which is the common 'LA_E' ID)\n",
                "# We also sort the index to ensure the order is correct after join\n",
                "merged_df = cm_data.join(submission_scores, how='inner').sort_index()\n",
                "\n",
                "# Reset index to get the 'LA_E' ID as a column again if needed\n",
                "merged_df.reset_index(inplace=True)\n",
                "\n",
                "# Rename the columns to have meaningful names\n",
                "merged_df.columns = ['LA_E_ID', 'Index', 'Key', 'Code', 'Label', 'Score']\n",
                "\n",
                "# Let's see the first few rows of the merged dataframe\n",
                "merged_df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "EER: 9.69% at threshold -0.45\n"
                    ]
                }
            ],
            "source": [
                "from evaluation import compute_eer\n",
                "bona_cm = merged_df[merged_df[\"Label\"] == 'bonafide']['Score'].values\n",
                "spoof_cm = merged_df[merged_df[\"Label\"] == 'spoof']['Score'].values\n",
                "\n",
                "eer_cm, th = compute_eer(bona_cm, spoof_cm)\n",
                "\n",
                "print(f'EER: {eer_cm:.2%} at threshold {th:.2f}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# In The Wild"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "ename": "ModuleNotFoundError",
                    "evalue": "No module named 'eval_metrics_DF'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01meval_metrics_DF\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mem\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meval_to_score_file\u001b[39m(score_file, cm_key_file):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# CM key file is the metadata file that contains the ground truth labels for the eval set\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# score file is the output of the system that contains the scores for the eval set\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# phase is the phase of the eval set (dev or eval)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     cm_data \u001b[38;5;241m=\u001b[39m pandas\u001b[38;5;241m.\u001b[39mread_csv(cm_key_file, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
                        "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'eval_metrics_DF'"
                    ]
                }
            ],
            "source": [
                "import sys\n",
                "import os.path\n",
                "import numpy as np\n",
                "import pandas\n",
                "import eval_metrics_DF as em\n",
                "\n",
                "\n",
                "def eval_to_score_file(score_file, cm_key_file):\n",
                "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
                "    # score file is the output of the system that contains the scores for the eval set\n",
                "    # phase is the phase of the eval set (dev or eval)\n",
                "\n",
                "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
                "\n",
                "    submission_scores = pandas.read_csv(\n",
                "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
                "    submission_scores[0] = submission_scores[0].astype(str)\n",
                "    submission_scores[0] = submission_scores[0].apply(\n",
                "        lambda x: x + '.wav' if not x.endswith('.wav') else x)\n",
                "\n",
                "    # check here for progress vs eval set\n",
                "    cm_scores = submission_scores.merge(\n",
                "        cm_data, left_on=0, right_on=0, how='inner')\n",
                "    # cm_scores.head()\n",
                "    #  0       1_x   1_y      2      3\n",
                "    #  a.wav  1.234   eval   Music   spoof\n",
                "    bona_cm = cm_scores[cm_scores['1_y'] == 'bonafide']['1_x'].values\n",
                "    spoof_cm = cm_scores[cm_scores['1_y'] == 'spoof']['1_x'].values\n",
                "\n",
                "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
                "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
                "    print(out_data)\n",
                "    return eer_cm\n",
                "\n",
                "score_file = '/data/hungdx/Towards-Real-Time-Deepfake-Speech-Detection-in-Resource-Limited-Scenarios/aasist_runs_datad/ConformerModel_kd_1s_r4/scores/ConformerModel_InTheWildnew_o_score__XLSR-6-Conformer_LA19-1s_conf-1-5_r4teacher_best125_new1s.txt'\n",
                "cm_key_file = '/datab/Dataset/cnsl_real_fake_audio/in_the_wild.txt'\n",
                "\n",
                "\n",
                "eval_to_score_file(score_file, cm_key_file)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Fake Or Real"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "eer: 9.108488707084048\tthreshold: -0.7349302768707275\n",
                        "\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "0.09108488707084048"
                        ]
                    },
                    "execution_count": 14,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import sys\n",
                "import os.path\n",
                "import numpy as np\n",
                "import pandas\n",
                "import eval_metrics_DF as em\n",
                "\n",
                "\n",
                "def eval_to_score_file(score_file, cm_key_file):\n",
                "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
                "    # score file is the output of the system that contains the scores for the eval set\n",
                "    # phase is the phase of the eval set (dev or eval)\n",
                "\n",
                "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
                "    \n",
                "    #print(cm_data.head())\n",
                "\n",
                "    submission_scores = pandas.read_csv(\n",
                "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
                "    \n",
                "    submission_scores[0] = submission_scores[0].astype(str)\n",
                "    submission_scores[0] = submission_scores[0].apply(\n",
                "        lambda x: x.split('/datab/Dataset/cnsl_real_fake_audio/fake_or_real/')[1])\n",
                "    #print(submission_scores.head())\n",
                "    # check here for progress vs eval set\n",
                "    cm_scores = submission_scores.merge(\n",
                "        cm_data, left_on=0, right_on=0, how='inner')\n",
                "    #print(cm_scores.head())\n",
                "    # cm_scores.head()\n",
                "    #  0       1_x   1_y      2      3\n",
                "    #  a.wav  1.234   eval   Music   spoof\n",
                "    bona_cm = cm_scores[cm_scores[2] == 'bonafide']['1_x'].values\n",
                "    spoof_cm = cm_scores[cm_scores[2] == 'spoof']['1_x'].values\n",
                "\n",
                "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
                "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
                "    print(out_data)\n",
                "    return eer_cm\n",
                "\n",
                "\n",
                "score_file = '/datad/hungdx/Towards-Real-Time-Deepfake-Speech-Detection-in-Resource-Limited-Scenarios/runs/aasist_runs/XLSR_AASIST_RawBoost3_random1strain/scores/XLSR_AASIST_DA3_FakeOrReal_score_first_epoch66_0.038874_98.9172.pt.txt'\n",
                "cm_key_file = '/datab/Dataset/cnsl_real_fake_audio/fake_or_real/protocol.txt'\n",
                "\n",
                "\n",
                "eval_to_score_file(score_file, cm_key_file)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Feb 07"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "/datad/hungdx/KDW2V-AASISTL/models/W2V2BASE_Linear_DKDLoss_noaudioaug_b16_randomstart_CosineAnnealingWarmRestarts_feb07/best_checkpoint_8.pth.txt\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "max score:  4.840700626373291\n",
                        "min score:  -4.0864996910095215\n",
                        "eer: 6.012596899224806\tthreshold: -2.3183629512786865\n",
                        "\n",
                        "/datad/hungdx/KDW2V-AASISTL/models/W2V2BASE_Linear_DKDLoss_noaudioaug_b16_randomstart_CosineAnnealingWarmRestarts_feb07/best_checkpoint_9.pth.txt\n",
                        "max score:  4.812656879425049\n",
                        "min score:  -4.110250949859619\n",
                        "eer: 6.099806201550387\tthreshold: -3.264273405075073\n",
                        "\n",
                        "/datad/hungdx/KDW2V-AASISTL/models/W2V2BASE_Linear_DKDLoss_noaudioaug_b16_randomstart_CosineAnnealingWarmRestarts_feb07/best_checkpoint_13.pth.txt\n",
                        "max score:  4.66938066482544\n",
                        "min score:  -3.947472095489502\n",
                        "eer: 5.5765503875969\tthreshold: -2.0495176315307617\n",
                        "\n",
                        "/datad/hungdx/KDW2V-AASISTL/models/W2V2BASE_Linear_DKDLoss_noaudioaug_b16_randomstart_CosineAnnealingWarmRestarts_feb07/best_checkpoint_27.pth.txt\n",
                        "max score:  4.807201385498047\n",
                        "min score:  -3.908827781677246\n",
                        "eer: 5.988372093023256\tthreshold: -3.642014980316162\n",
                        "\n",
                        "/datad/hungdx/KDW2V-AASISTL/models/W2V2BASE_Linear_DKDLoss_noaudioaug_b16_randomstart_CosineAnnealingWarmRestarts_feb07/best_checkpoint_4.pth.txt\n",
                        "max score:  4.705055236816406\n",
                        "min score:  -4.015733242034912\n",
                        "eer: 5.857558139534884\tthreshold: -1.7340211868286133\n",
                        "\n",
                        "/datad/hungdx/KDW2V-AASISTL/models/W2V2BASE_Linear_DKDLoss_noaudioaug_b16_randomstart_CosineAnnealingWarmRestarts_feb07/best_checkpoint_7.pth.txt\n",
                        "max score:  4.81353759765625\n",
                        "min score:  -4.063755035400391\n",
                        "eer: 5.7558139534883725\tthreshold: -2.023259401321411\n",
                        "\n",
                        "/datad/hungdx/KDW2V-AASISTL/models/W2V2BASE_Linear_DKDLoss_noaudioaug_b16_randomstart_CosineAnnealingWarmRestarts_feb07/best_checkpoint_50.pth.txt\n",
                        "max score:  4.659171581268311\n",
                        "min score:  -3.8557162284851074\n",
                        "eer: 5.082364341085271\tthreshold: -3.520453929901123\n",
                        "\n",
                        "/datad/hungdx/KDW2V-AASISTL/models/W2V2BASE_Linear_DKDLoss_noaudioaug_b16_randomstart_CosineAnnealingWarmRestarts_feb07/best_checkpoint_1.pth.txt\n",
                        "max score:  3.5357346534729004\n",
                        "min score:  -3.7066030502319336\n",
                        "eer: 7.054263565891474\tthreshold: -1.0857607126235962\n",
                        "\n",
                        "/datad/hungdx/KDW2V-AASISTL/models/W2V2BASE_Linear_DKDLoss_noaudioaug_b16_randomstart_CosineAnnealingWarmRestarts_feb07/best_checkpoint_2.pth.txt\n",
                        "max score:  4.040817260742188\n",
                        "min score:  -3.942868947982788\n",
                        "eer: 7.117248062015504\tthreshold: -1.1284130811691284\n",
                        "\n",
                        "/datad/hungdx/KDW2V-AASISTL/models/W2V2BASE_Linear_DKDLoss_noaudioaug_b16_randomstart_CosineAnnealingWarmRestarts_feb07/best_checkpoint_0.pth.txt\n",
                        "max score:  2.1109766960144043\n",
                        "min score:  -2.3592989444732666\n",
                        "eer: 9.689922480620156\tthreshold: 0.6020740270614624\n",
                        "\n",
                        "/datad/hungdx/KDW2V-AASISTL/models/W2V2BASE_Linear_DKDLoss_noaudioaug_b16_randomstart_CosineAnnealingWarmRestarts_feb07/best_checkpoint_3.pth.txt\n",
                        "max score:  4.472354412078857\n",
                        "min score:  -3.975835084915161\n",
                        "eer: 7.635658914728682\tthreshold: -0.6498187780380249\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "import sys\n",
                "import os.path\n",
                "import numpy as np\n",
                "import pandas\n",
                "import eval_metrics_DF as em\n",
                "import os\n",
                "\n",
                "def eval_to_score_file(score_file, cm_key_file, phase):\n",
                "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
                "    # score file is the output of the system that contains the scores for the eval set\n",
                "    # phase is the phase of the eval set (dev or eval)\n",
                "\n",
                "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
                "    # \n",
                "\n",
                "    submission_scores = pandas.read_csv(\n",
                "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
                "    \n",
                "    \n",
                "    # submission_scores[0] = submission_scores[0].apply(\n",
                "    #     lambda x: 'eval/' + x +'.wav')\n",
                "    \n",
                "    # print(submission_scores[0].head())\n",
                "    \n",
                "    # check here for progress vs eval set\n",
                "    cm_scores = submission_scores.merge(\n",
                "        cm_data[cm_data[1] == phase], left_on=0, right_on=0, how='inner')\n",
                "    \n",
                "    \n",
                "    # cm_scores.head()\n",
                "    #  0       1_x   1_y      2      3\n",
                "    #  a.wav  1.234   eval   Music   spoof\n",
                "    bona_cm = cm_scores[cm_scores[3] == 'bonafide']['1_x'].values\n",
                "    spoof_cm = cm_scores[cm_scores[3] == 'spoof']['1_x'].values\n",
                "\n",
                "    max_score = max(cm_scores['1_x'].values)\n",
                "    min_score = min(cm_scores['1_x'].values)\n",
                "\n",
                "    print(\"max score: \", max_score)\n",
                "    print(\"min score: \", min_score)\n",
                "\n",
                "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
                "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
                "    print(out_data)\n",
                "    return eer_cm\n",
                "\n",
                "\n",
                "SCORE_DIR = '/datad/hungdx/KDW2V-AASISTL/models/W2V2BASE_Linear_DKDLoss_noaudioaug_b16_randomstart_CosineAnnealingWarmRestarts_feb07'\n",
                "cm_key_file = '/datab/Dataset/cnsl_real_fake_audio/supcon_cnsl_feb07/protocol.txt'\n",
                "for file in os.listdir(SCORE_DIR):\n",
                "    if file.endswith(\".txt\"):\n",
                "        score_file = os.path.join(SCORE_DIR, file)\n",
                "        print(score_file)\n",
                "        try:\n",
                "            eval_to_score_file(score_file, cm_key_file, 'eval')\n",
                "        except:\n",
                "            pass"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Feb07 Sepecial\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "metadata": {},
            "outputs": [
                {
                    "ename": "FileNotFoundError",
                    "evalue": "[Errno 2] No such file or directory: '/datad/utils/vocodetect/Supcon-voco/model_weighted_CCE_200_1_1e-07_5_augall_wav2vecbase_linear_feb07_KD_lst_e67.txt'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[40], line 55\u001b[0m\n\u001b[1;32m     50\u001b[0m cm_key_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/datab/Dataset/cnsl_real_fake_audio/supcon_cnsl_feb07/protocol.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     52\u001b[0m score_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/datad/utils/vocodetect/Supcon-voco/model_weighted_CCE_200_1_1e-07_5_augall_wav2vecbase_linear_feb07_KD_lst_e67.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 55\u001b[0m \u001b[43meval_to_score_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscore_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcm_key_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meval\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
                        "Cell \u001b[0;32mIn[40], line 17\u001b[0m, in \u001b[0;36meval_to_score_file\u001b[0;34m(score_file, cm_key_file, phase)\u001b[0m\n\u001b[1;32m     14\u001b[0m cm_data \u001b[38;5;241m=\u001b[39m pandas\u001b[38;5;241m.\u001b[39mread_csv(cm_key_file, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m submission_scores \u001b[38;5;241m=\u001b[39m \u001b[43mpandas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscore_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipinitialspace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m submission_scores[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m submission_scores[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m x)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Drop last column from submission_scores\u001b[39;00m\n",
                        "File \u001b[0;32m~/miniconda3/envs/rawformer/lib/python3.8/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/miniconda3/envs/rawformer/lib/python3.8/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
                        "File \u001b[0;32m~/miniconda3/envs/rawformer/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/miniconda3/envs/rawformer/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
                        "File \u001b[0;32m~/miniconda3/envs/rawformer/lib/python3.8/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
                        "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/datad/utils/vocodetect/Supcon-voco/model_weighted_CCE_200_1_1e-07_5_augall_wav2vecbase_linear_feb07_KD_lst_e67.txt'"
                    ]
                }
            ],
            "source": [
                "import sys\n",
                "import os.path\n",
                "import numpy as np\n",
                "import pandas\n",
                "import eval_metrics_DF as em\n",
                "import os\n",
                "\n",
                "\n",
                "def eval_to_score_file(score_file, cm_key_file, phase):\n",
                "    # CM key file is the metadata file that contains the ground truth labels for the eval set\n",
                "    # score file is the output of the system that contains the scores for the eval set\n",
                "    # phase is the phase of the eval set (dev or eval)\n",
                "\n",
                "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
                "    #\n",
                "\n",
                "    submission_scores = pandas.read_csv(\n",
                "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
                "\n",
                "    submission_scores[0] = submission_scores[0].apply(\n",
                "        lambda x: 'eval/' + x)\n",
                "    \n",
                "    # Drop last column from submission_scores\n",
                "    \n",
                "\n",
                "    print(submission_scores[0].head())\n",
                "\n",
                "    # check here for progress vs eval set\n",
                "    cm_scores = submission_scores.merge(\n",
                "        cm_data[cm_data[1] == phase], left_on=0, right_on=0, how='inner')\n",
                "    print(cm_scores.head())\n",
                "    # cm_scores.head()\n",
                "    #  0       1_x   1_y      2      3\n",
                "    #  a.wav  1.234   eval   Music   spoof\n",
                "    bona_cm = cm_scores[cm_scores[3] == 'bonafide']['2_x'].values\n",
                "    spoof_cm = cm_scores[cm_scores[3] == 'spoof']['2_x'].values\n",
                "\n",
                "    max_score = max(cm_scores['2_x'].values)\n",
                "    min_score = min(cm_scores['2_x'].values)\n",
                "\n",
                "    print(\"max score: \", max_score)\n",
                "    print(\"min score: \", min_score)\n",
                "\n",
                "    eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
                "    out_data = \"eer: {}\\tthreshold: {}\\n\".format(100*eer_cm, th)\n",
                "    print(out_data)\n",
                "    return eer_cm\n",
                "\n",
                "\n",
                "cm_key_file = '/datab/Dataset/cnsl_real_fake_audio/supcon_cnsl_feb07/protocol.txt'\n",
                "\n",
                "score_file = os.path.join(\n",
                "    \"/datad/utils/vocodetect/Supcon-voco/model_weighted_CCE_200_1_1e-07_5_augall_wav2vecbase_linear_feb07_KD_lst_e67.txt\")\n",
                "        \n",
                "eval_to_score_file(score_file, cm_key_file, 'eval')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Score fusion (Averaging)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import os.path\n",
                "import numpy as np\n",
                "import pandas\n",
                "import eval_metrics_DF as em\n",
                "import os\n",
                "\n",
                "list_score_files = [\n",
                "    # '/datad/hungdx/KDW2V-AASISTL/byot_runs/Distil_XLSR_5_Custom_Trans_Layer_VIB_aug3_b16_randomstart_feb07_byot_from_best_checkpoint/best_checkpoint_5.pth_classifer_0.txt',\n",
                "    # '/datad/hungdx/KDW2V-AASISTL/byot_runs/Distil_XLSR_5_Custom_Trans_Layer_VIB_aug3_b16_randomstart_feb07_byot_from_best_checkpoint/best_checkpoint_5.pth_classifer_1.txt',\n",
                "    # '/datad/hungdx/KDW2V-AASISTL/byot_runs/Distil_XLSR_5_Custom_Trans_Layer_VIB_aug3_b16_randomstart_feb07_byot_from_best_checkpoint/best_checkpoint_5.pth_classifer_2.txt',\n",
                "    '/data/hungdx/Towards-Real-Time-Deepfake-Speech-Detection-in-Resource-Limited-Scenarios/runs/aasist_runs/XLSR_AASIST_RawBoost4/scores/XLSR_AASIST_DA3_EMPHASIS_DF21_score_best32.txt',\n",
                "    '/datad/hungdx/Rawformer-implementation-anti-spoofing/results/Conformer*/DF21/random1s/ConformerModel_BASELINE_DF21_random1s_score__random1s_best_50.txt',\n",
                "]\n",
                "\n",
                "\n",
                "submission_scores_list = [\n",
                "    pandas.read_csv(\n",
                "        score_file, sep=' ', header=None, skipinitialspace=True) for score_file in list_score_files\n",
                "\n",
                "]\n",
                "\n",
                "\n",
                "# Calculate average score from all submission_scores at 1th column\n",
                "average_score = pandas.concat(\n",
                "    [submission_scores[1] for submission_scores in submission_scores_list], axis=1).mean(axis=1)\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "# average_score = (submission_scores1[1] + submission_scores2[1]) / 2\n",
                "final_score = pandas.concat([submission_scores_list[0][0], average_score], axis=1)\n",
                "\n",
                "# Save final_score to a txt file\n",
                "final_score.to_csv('XLSRraw4_best32_and_Conformer_best50_1s_fusion.txt',\n",
                "                   sep=' ', header=False, index=False)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Breakdown Feb 07"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas\n",
                "import sys\n",
                "import os.path\n",
                "import numpy as np\n",
                "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
                "import matplotlib.pyplot as plt\n",
                "import eval_metrics_DF as em\n",
                "from urllib.parse import urlparse\n",
                "\n",
                "\n",
                "def is_valid_url(url):\n",
                "    try:\n",
                "        result = urlparse(url)\n",
                "        return all([result.scheme, result.netloc])\n",
                "    except:\n",
                "        return False\n",
                "\n",
                "\n",
                "def extract_domain(url):\n",
                "    # print(\"Extracting domain from URL: \", url)\n",
                "\n",
                "    # Check if the url is not valid url\n",
                "    if not is_valid_url(url):\n",
                "        return url\n",
                "\n",
                "    if not isinstance(url, str):\n",
                "        return None\n",
                "    netloc = urlparse(url).netloc\n",
                "    # If the URL is a short version of youtube URL (e.g., youtu.be), handle it\n",
                "    if 'youtu.be' in netloc or 'youtube.com' in netloc:\n",
                "        return 'youtube.com'\n",
                "    # Extract the domain name (e.g., example.com from https://www.example.com/path)\n",
                "    # Keep subdomains (e.g., sub.example.com)\n",
                "    domain = netloc\n",
                "\n",
                "    # domain = netloc.split('.')[-2] + '.' + netloc.split('.')[-1]\n",
                "    return domain\n",
                "\n",
                "\n",
                "def eval_to_score_file(score_file, cm_key_file, meta_file, phase):\n",
                "    # Load the CM key file and the submission scores\n",
                "    cm_data = pandas.read_csv(cm_key_file, sep=' ', header=None)\n",
                "    submission_scores = pandas.read_csv(\n",
                "        score_file, sep=' ', header=None, skipinitialspace=True)\n",
                "    \n",
                "    submission_scores[0] = submission_scores[0].apply(\n",
                "        lambda x: 'eval/' + x + '.wav')\n",
                "\n",
                "    # Filter CM data for the specific phase\n",
                "    cm_scores = submission_scores.merge(\n",
                "        cm_data[cm_data[1] == phase], left_on=0, right_on=0, how='inner')\n",
                "\n",
                "    print(\"Number of scores for phase {}: {}\".format(phase, len(cm_scores)))\n",
                "    # Load meta data\n",
                "    meta_data = pandas.read_csv(meta_file, sep=',')\n",
                "\n",
                "    # Prepare meta_data by concatenating 'eval/' with the 'Utterence name (file name)' to match cm_scores format\n",
                "    meta_data[\"file_name\"] = 'eval/' + meta_data[\"utt\"] + '.wav'\n",
                "\n",
                "    # Add a 'domain' column to the meta_data by applying the extract_domain function\n",
                "    meta_data['domain'] = meta_data['Source link'].apply(extract_domain)\n",
                "\n",
                "    # Merge cm_scores with meta_data\n",
                "    merged_data = cm_scores.merge(\n",
                "        meta_data, left_on=0, right_on=\"file_name\", how='inner')\n",
                "\n",
                "    print(\"Number of scores for phase {} after merging with meta data: {}\".format(\n",
                "        phase, len(merged_data)))\n",
                "\n",
                "    # Now you can group by the \"group\" column in meta_data for further analysis\n",
                "    grouped = merged_data.groupby(\"group\")\n",
                "\n",
                "    # Example of iterating over each group and computing EER for each\n",
                "\n",
                "    group_dict = {}\n",
                "    for group_name, group_data in grouped:\n",
                "\n",
                "        bona_cm = group_data[group_data[3] == 'bonafide']['1_x'].values\n",
                "        spoof_cm = group_data[group_data[3] == 'spoof']['1_x'].values\n",
                "\n",
                "        number_of_bona = len(bona_cm)\n",
                "        number_of_spoof = len(spoof_cm)\n",
                "\n",
                "        # Calculate mis classifcation like false positive because we are interested in spoof detection and because we just got bona_cm only\n",
                "\n",
                "        if len(bona_cm) > 0 and len(spoof_cm) > 0:  # Ensure there's data to compute EER\n",
                "            eer_cm, th = em.compute_eer(bona_cm, spoof_cm)\n",
                "            out_data = \"Group: {}\\tnum_bona:{}\\tnum_spoof:{}\\teer: {}\\tthreshold: {}\\n\".format(\n",
                "                group_name, number_of_bona, number_of_spoof,  100*eer_cm, th)\n",
                "\n",
                "            # Draw the confusion matrix\n",
                "            y_true = np.concatenate(\n",
                "                (np.zeros(len(bona_cm)), np.ones(len(spoof_cm))))\n",
                "            y_score = np.concatenate((bona_cm, spoof_cm))\n",
                "\n",
                "            # Score < th is predicted as spoof (negative class) and score >= th is predicted as bona (positive class)\n",
                "\n",
                "            cm = confusion_matrix(y_true, y_score < th)\n",
                "\n",
                "            # Calculate number of false positives, false negatives\n",
                "            fn = cm[0][1]\n",
                "            fp = cm[1][0]\n",
                "\n",
                "            disp = ConfusionMatrixDisplay(\n",
                "                confusion_matrix=cm, display_labels=[\"bonafide\", \"spoof\"])\n",
                "            disp.plot()\n",
                "            plt.title(\"Group: {}\".format(group_name))\n",
                "            plt.show()\n",
                "\n",
                "            print(out_data)\n",
                "            group_dict[group_name] = [100*eer_cm, th, fp, fn]\n",
                "\n",
                "    # Return value can be adjusted based on your needs\n",
                "    return group_dict, grouped"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for file in os.listdir(SCORE_DIR):\n",
                "    if file.endswith(\".txt\"):\n",
                "        score_file = os.path.join(SCORE_DIR, file)\n",
                "        cm_key_file = '/datab/Dataset/cnsl_real_fake_audio/supcon_cnsl_feb07/protocol.txt'\n",
                "        meta_file = '/datab/Dataset/cnsl_real_fake_audio/supcon_cnsl_feb07/df_meta_feb07.csv'\n",
                "        print(score_file)\n",
                "        eval_to_score_file(score_file, cm_key_file, meta_file,'eval')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# DF 21"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "rawformer",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.13"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
